<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>src.NLP.NLPPredictor.predictor API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.NLP.NLPPredictor.predictor</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import pandas as pd
import numpy as np
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import gensim
import pickle
import keras
from keras_preprocessing.sequence import pad_sequences
from src.NLP.NLPPredictor.preprocessing_functions import *
import csv


def preprocess_nlp(text):
    &#34;&#34;&#34;
    Preprocesses the given text for natural language processing (NLP) tasks.

    Args:
        text (str): The input text to be preprocessed.

    Returns:
        csv: A list of sentences, where each sentence is a list of words.

    Example:
        &gt;&gt;&gt; text = &#34;This is a sample text.&#34;
        &gt;&gt;&gt; preprocess_nlp(text)
        [[&#39;this&#39;, &#39;is&#39;, &#39;a&#39;, &#39;sample&#39;, &#39;text&#39;]]
    &#34;&#34;&#34;
    output_text = remove_newlines_tabs(text)
    output_text = strip_html_tags(output_text)
    output_text = remove_links(output_text)
    output_text = expand_contractions(output_text)
    output_text = remove_whitespace(output_text)
    output_text = reducing_incorrect_character_repetition(output_text)
    output_text = lower_casing_text(output_text)
    output_text = removing_special_characters(output_text)

    # string to list of strings
    if isinstance(output_text, str):
        output_text = output_text.split()

    # turn a list of words into a list of sentences where each sentence is a list of words
    output_list = manual_words_to_sentences(output_text)

    # remove empty list entries
    output_list = list(filter(None, output_list))

    return output_list


def process_split_words(text):
    &#34;&#34;&#34;

    :param text: plain text
    :return: array of tokenized words
    &#34;&#34;&#34;
    result = []
    #nltk.download(&#39;punkt&#39;, quiet=True)  # Download the necessary tokenizer data
    sentences = nltk.sent_tokenize(text)
    for sentence in sentences:
        words = sentence.split()
        result.append(words)
    return result


def predictor(text):
    &#34;&#34;&#34;

    :return: dictionary with frequency of each class appears in the given text
    :param text: plain text
    &#34;&#34;&#34;
    result = process_split_words(text)
    # Load the tokenizer
    # Try abs path if it does not work 
    with open(&#39;/app/src/NLP/NLPPredictor/f_tokenizer.pkl&#39;, &#39;rb&#39;) as f:
    #with open(&#39;tokenizer.pkl&#39;, &#39;rb&#39;) as f:
        tokenizer = pickle.load(f)
    sequences = tokenizer.texts_to_sequences(result)
    pad_rev = pad_sequences(sequences, maxlen=192, padding=&#39;post&#39;)
    # Try abs path if it does not work
    lstm_model = keras.models.load_model(&#39;/app/src/NLP/NLPPredictor/f_bi_lstm_model.h5&#39;)
    #lstm_model = keras.models.load_model(&#39;Bidirectional_LSTM_Model.h5&#39;)
    preds = lstm_model.predict(pad_rev)
    classes = []

    for pred in preds:
        if pred[np.argmax(pred)] &gt; 0.5:
            classes.append(np.argmax(pred))

    result = {}
    mapping = [&#39;Data Recipients&#39;, &#39;Safeguards Copy&#39;, &#39;Processing Purpose&#39;,
               &#39;Data Categories&#39;, &#39;Source of Data&#39;, &#39;Right to Erase&#39;,
               &#39;Right to Restrict&#39;, &#39;Right to Access&#39;, &#39;Right to Object&#39;,
               &#39;Withdraw Consent&#39;, &#39;Right to Portability&#39;, &#39;Profiling&#39;,
               &#39;Controller Contact&#39;, &#39;Provision Requirement&#39;,
               &#39;Storage Period&#39;, &#39;Lodge Complaint&#39;, &#39;DPO Contact&#39;,
               &#39;Adequacy Decision&#39;]
    for cat in mapping:
        result[cat] = 0
    for c in classes:
        result[mapping[c]] = 1
    return result


def save_list_as_csv(data_list, filename):
    with open(filename, &#39;w&#39;, newline=&#39;&#39;) as file:
        writer = csv.writer(file)

        # Write each sentence as a row in the CSV file
        for elem in data_list:
            writer.writerow(elem)


def text_file_to_csv_file(file_path):
    # Open the file in read mode
    file = open(file_path, &#34;r&#34;, encoding=&#34;utf-8&#34;)

    # Read the entire content of the file
    input_text = file.read()

    text = preprocess_nlp(input_text)

    save_list_as_csv(text, &#39;output.csv&#39;)


if __name__ == &#39;__main__&#39;:
    text_file_to_csv_file(&#34;example_policy.txt&#34;)
    # preds = process_split_words(&#34;sd asd awd  awfawf  saffaw. awda awd s d wad ? adwuidandnwda adina dawd, awdjajwd.&#34;)
    # preds = predictor(&#34;We may provide our analysis and certain non-personal information to third parties who may in turn use this information to provide advertisements tailored to your interests. We seek to maintain the integrity and security of your Personal Information. In order to improve guest online and mobile shopping experiences, help with fraud identification. These contracts give your personal data the same protection it has in the EEA. We may share some or all of your personal data with our affiliates.&#34;)
    # print(preds)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="src.NLP.NLPPredictor.predictor.predictor"><code class="name flex">
<span>def <span class="ident">predictor</span></span>(<span>text)</span>
</code></dt>
<dd>
<div class="desc"><p>:return: dictionary with frequency of each class appears in the given text
:param text: plain text</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predictor(text):
    &#34;&#34;&#34;

    :return: dictionary with frequency of each class appears in the given text
    :param text: plain text
    &#34;&#34;&#34;
    result = process_split_words(text)
    # Load the tokenizer
    # Try abs path if it does not work 
    with open(&#39;/app/src/NLP/NLPPredictor/f_tokenizer.pkl&#39;, &#39;rb&#39;) as f:
    #with open(&#39;tokenizer.pkl&#39;, &#39;rb&#39;) as f:
        tokenizer = pickle.load(f)
    sequences = tokenizer.texts_to_sequences(result)
    pad_rev = pad_sequences(sequences, maxlen=192, padding=&#39;post&#39;)
    # Try abs path if it does not work
    lstm_model = keras.models.load_model(&#39;/app/src/NLP/NLPPredictor/f_bi_lstm_model.h5&#39;)
    #lstm_model = keras.models.load_model(&#39;Bidirectional_LSTM_Model.h5&#39;)
    preds = lstm_model.predict(pad_rev)
    classes = []

    for pred in preds:
        if pred[np.argmax(pred)] &gt; 0.5:
            classes.append(np.argmax(pred))

    result = {}
    mapping = [&#39;Data Recipients&#39;, &#39;Safeguards Copy&#39;, &#39;Processing Purpose&#39;,
               &#39;Data Categories&#39;, &#39;Source of Data&#39;, &#39;Right to Erase&#39;,
               &#39;Right to Restrict&#39;, &#39;Right to Access&#39;, &#39;Right to Object&#39;,
               &#39;Withdraw Consent&#39;, &#39;Right to Portability&#39;, &#39;Profiling&#39;,
               &#39;Controller Contact&#39;, &#39;Provision Requirement&#39;,
               &#39;Storage Period&#39;, &#39;Lodge Complaint&#39;, &#39;DPO Contact&#39;,
               &#39;Adequacy Decision&#39;]
    for cat in mapping:
        result[cat] = 0
    for c in classes:
        result[mapping[c]] = 1
    return result</code></pre>
</details>
</dd>
<dt id="src.NLP.NLPPredictor.predictor.preprocess_nlp"><code class="name flex">
<span>def <span class="ident">preprocess_nlp</span></span>(<span>text)</span>
</code></dt>
<dd>
<div class="desc"><p>Preprocesses the given text for natural language processing (NLP) tasks.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>text</code></strong> :&ensp;<code>str</code></dt>
<dd>The input text to be preprocessed.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>csv</code></dt>
<dd>A list of sentences, where each sentence is a list of words.</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; text = &quot;This is a sample text.&quot;
&gt;&gt;&gt; preprocess_nlp(text)
[['this', 'is', 'a', 'sample', 'text']]
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def preprocess_nlp(text):
    &#34;&#34;&#34;
    Preprocesses the given text for natural language processing (NLP) tasks.

    Args:
        text (str): The input text to be preprocessed.

    Returns:
        csv: A list of sentences, where each sentence is a list of words.

    Example:
        &gt;&gt;&gt; text = &#34;This is a sample text.&#34;
        &gt;&gt;&gt; preprocess_nlp(text)
        [[&#39;this&#39;, &#39;is&#39;, &#39;a&#39;, &#39;sample&#39;, &#39;text&#39;]]
    &#34;&#34;&#34;
    output_text = remove_newlines_tabs(text)
    output_text = strip_html_tags(output_text)
    output_text = remove_links(output_text)
    output_text = expand_contractions(output_text)
    output_text = remove_whitespace(output_text)
    output_text = reducing_incorrect_character_repetition(output_text)
    output_text = lower_casing_text(output_text)
    output_text = removing_special_characters(output_text)

    # string to list of strings
    if isinstance(output_text, str):
        output_text = output_text.split()

    # turn a list of words into a list of sentences where each sentence is a list of words
    output_list = manual_words_to_sentences(output_text)

    # remove empty list entries
    output_list = list(filter(None, output_list))

    return output_list</code></pre>
</details>
</dd>
<dt id="src.NLP.NLPPredictor.predictor.process_split_words"><code class="name flex">
<span>def <span class="ident">process_split_words</span></span>(<span>text)</span>
</code></dt>
<dd>
<div class="desc"><p>:param text: plain text
:return: array of tokenized words</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_split_words(text):
    &#34;&#34;&#34;

    :param text: plain text
    :return: array of tokenized words
    &#34;&#34;&#34;
    result = []
    #nltk.download(&#39;punkt&#39;, quiet=True)  # Download the necessary tokenizer data
    sentences = nltk.sent_tokenize(text)
    for sentence in sentences:
        words = sentence.split()
        result.append(words)
    return result</code></pre>
</details>
</dd>
<dt id="src.NLP.NLPPredictor.predictor.save_list_as_csv"><code class="name flex">
<span>def <span class="ident">save_list_as_csv</span></span>(<span>data_list, filename)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_list_as_csv(data_list, filename):
    with open(filename, &#39;w&#39;, newline=&#39;&#39;) as file:
        writer = csv.writer(file)

        # Write each sentence as a row in the CSV file
        for elem in data_list:
            writer.writerow(elem)</code></pre>
</details>
</dd>
<dt id="src.NLP.NLPPredictor.predictor.text_file_to_csv_file"><code class="name flex">
<span>def <span class="ident">text_file_to_csv_file</span></span>(<span>file_path)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def text_file_to_csv_file(file_path):
    # Open the file in read mode
    file = open(file_path, &#34;r&#34;, encoding=&#34;utf-8&#34;)

    # Read the entire content of the file
    input_text = file.read()

    text = preprocess_nlp(input_text)

    save_list_as_csv(text, &#39;output.csv&#39;)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src.NLP.NLPPredictor" href="index.html">src.NLP.NLPPredictor</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="src.NLP.NLPPredictor.predictor.predictor" href="#src.NLP.NLPPredictor.predictor.predictor">predictor</a></code></li>
<li><code><a title="src.NLP.NLPPredictor.predictor.preprocess_nlp" href="#src.NLP.NLPPredictor.predictor.preprocess_nlp">preprocess_nlp</a></code></li>
<li><code><a title="src.NLP.NLPPredictor.predictor.process_split_words" href="#src.NLP.NLPPredictor.predictor.process_split_words">process_split_words</a></code></li>
<li><code><a title="src.NLP.NLPPredictor.predictor.save_list_as_csv" href="#src.NLP.NLPPredictor.predictor.save_list_as_csv">save_list_as_csv</a></code></li>
<li><code><a title="src.NLP.NLPPredictor.predictor.text_file_to_csv_file" href="#src.NLP.NLPPredictor.predictor.text_file_to_csv_file">text_file_to_csv_file</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>